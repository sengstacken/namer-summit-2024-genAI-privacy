{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9691ea2b-3ee8-4742-9ff1-ca933e1bc72d",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock\n",
    "## Access Control Filtering\n",
    "\n",
    "This notebook guides users through creating and implementing access controls for Knowledge Bases on Amazon Bedrock. Amazon Bedrock is a fully managed service that provides easy access to foundation models (FMs) from leading AI companies through a single API, allowing you to build and scale generative AI applications quickly and easily.\n",
    "\n",
    "This notebook demonstrates a practical use case for a large enterprise, AcmeCorp, showcasing how to restrict data access based on user roles using a Retrieval Augmented Generation (RAG) architecture.\n",
    "\n",
    "### Use Case Overview\n",
    "At AcmeCorp, we aim to create a Knowledge Base containing content. However, not all users have access to all data. This notebook will walk you through setting up a RAG system that restricts retrieval to only the documents a user has permission to access. We will implement this using metadata filtering with Amazon Bedrock Knowledge Bases.\n",
    "\n",
    "### Prerequisites\n",
    "***ADD info here about what has been set up via workshop studio***\n",
    "\n",
    "### Notebook Sections\n",
    "1. Amazon Cognito User Management\n",
    "2. User-Corpus Association in Amazon DynamoDB\n",
    "3. Dataset Download and Preparation\n",
    "4. Metadata Association\n",
    "5. Create OpenSearch Serverless Collection\n",
    "6. Create and Configure Knowledge Base for Amazon Bedrock\n",
    "7. Update AWS Lambda\n",
    "8. Create and Run a Streamlit Application\n",
    "9. Clean-up Resources\n",
    "\n",
    "Each section will guide you through the process of setting up and configuring the necessary components for our metadata-filtered chat interface.\n",
    "\n",
    "This notebook was built and tested using the `conda_python3` notebook kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c10f35-09a3-4e9c-8f73-e9235609ac0b",
   "metadata": {},
   "source": [
    "Let's import necessary Python modules and libraries, and initialize AWS service clients required for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51edb0-cae2-4929-b0a5-8a8057c7f467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU opensearch-py streamlit streamlit-cognito-auth retrying boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3502ca-2256-499d-9d37-70021e379a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import requests\n",
    "import random\n",
    "import mimetypes\n",
    "\n",
    "from utilsmod import create_base_infrastructure, create_kb_infrastructure, updateDataAccessPolicy, createAOSSIndex, replace_vars\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "lambda_client = boto3.client('lambda')\n",
    "dynamodb_resource = boto3.resource('dynamodb')\n",
    "cloudformation = boto3.client('cloudformation')\n",
    "opensearch = boto3.client('opensearchserverless')\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock = boto3.client(\"bedrock\",region_name=region)\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "cognito_client = boto3.client('cognito-idp', region_name=region)\n",
    "identity_arn = session.client('sts').get_caller_identity()['Arn']\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')\n",
    "bucket_name = 'namer-' + account_id + '-bucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519d96-1fd1-43e9-be5a-d8a766fda04f",
   "metadata": {},
   "source": [
    "Get outputs from Cloudformation to use later in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739b819-9846-4a71-85bf-e6e3e9e5d994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stack = cloudformation.describe_stacks(\n",
    "    StackName='Namer'\n",
    ")\n",
    "\n",
    "outputs = stack[\"Stacks\"][0][\"Outputs\"]\n",
    "for output in outputs:\n",
    "    keyName = output[\"OutputKey\"]\n",
    "    if keyName == \"clientid\":\n",
    "        clientid = output[\"OutputValue\"]\n",
    "    elif keyName == \"clientsecret\":\n",
    "        clientsecret = output[\"OutputValue\"]\n",
    "    elif keyName == \"cognitoarn\":\n",
    "        cognitoarn = output[\"OutputValue\"]\n",
    "    elif keyName == \"dynamotable\":\n",
    "        dynamotable = output[\"OutputValue\"]\n",
    "    elif keyName == \"lambdafunctionarn\":\n",
    "        lambdafunctionarn = output[\"OutputValue\"]\n",
    "    elif keyName == \"s3bucket\":\n",
    "        s3bucket = output[\"OutputValue\"]\n",
    "    elif keyName == \"userpoolarn\":\n",
    "        userpoolarn = output[\"OutputValue\"]\n",
    "    elif keyName == \"userpoolid\":\n",
    "        userpoolid = output[\"OutputValue\"]\n",
    "    elif keyName == \"VPC\":\n",
    "        VPC = output[\"OutputValue\"]\n",
    "\n",
    "print(\"clientid: \" + clientid)\n",
    "print(\"clientsecret: \" + clientsecret)\n",
    "print(\"cognitoarn: \" + cognitoarn)\n",
    "print(\"dynamotable: \" + dynamotable)\n",
    "print(\"lambdafunctionarn: \" + lambdafunctionarn)\n",
    "print(\"s3bucket: \" + s3bucket)\n",
    "print(\"userpoolarn: \" + userpoolarn)\n",
    "print(\"userpoolid: \" + userpoolid)\n",
    "print(\"VPC: \" + VPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfb833-f538-4bf7-8b9a-4ebeb66710b9",
   "metadata": {},
   "source": [
    "### 0. VPC to Bedrock Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9d582-c152-4eac-9b03-719f61cbbeeb",
   "metadata": {},
   "source": [
    "In this section, we use Amazon Virtual Private Cloud (Amazon VPC) to set up the VPC endpoint for Amazon Bedrock to facilitate private connectivity from your VPC to Amazon Bedrock.\n",
    "\n",
    "1. On the Amazon VPC console, under Virtual private cloud in the navigation pane, choose Endpoints.\n",
    "2. Choose Create endpoint.\n",
    "\n",
    "\n",
    "3. For Name tag, enter bedrock-vpce.\n",
    "4. Under Services, search for bedrock-runtime and select com.amazonaws.<region>.bedrock-agent-runtime.  Where region is us-west-2\n",
    "5. For VPC, specify the VPC Bedrock-GenAI-Project-vpc that you created through the CloudFormation stack in the previous section.\n",
    "6. In the Subnets section, and select the Availability Zones and choose the corresponding subnet IDs from the drop-down menu.\n",
    "7. For Security groups, select the security group with the group name Bedrock-GenAI-Stack-VPCEndpointSecurityGroup- and description Allow TLS for VPC Endpoint.  A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. Note that this VPC endpoint security group only allows traffic originating from the security group attached to your VPC private subnets, adding a layer of protection.\n",
    "8. Choose Create endpoint.\n",
    "9. In the Policy section, select Custom and enter the following least privilege policy to ensure only certain actions are allowed on the specified foundation model resource for a given principal (such as Lambda function IAM role).\n",
    "```\n",
    "{\n",
    "\t\"Version\": \"2012-10-17\",\n",
    "\t\"Statement\": [\n",
    "\t\t{\n",
    "\t\t    \"Action\": [\n",
    "\t\t        \"bedrock:RetrieveAndGenerate\"\n",
    "\t\t        ],\n",
    "\t\t    \"Resource\": [\n",
    "\t\t        \"*\"\n",
    "\t\t        ],\n",
    "\t\t    \"Effect\": \"Allow\",\n",
    "\t\t    \"Principal\": {\n",
    "                \"AWS\": \"arn:aws:iam::<accountid>:role/<region>-<accountId>-SageMaker-Execution-Namer-2024-Role\"\n",
    "            }\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n",
    "It may take up to 2 minutes until the interface endpoint is created and the status changes to Available. You can refresh the page to check the latest status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40293544-d50a-4bb9-8a4f-e7700590200e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Amazon Cognito User Pool: Users and Corpus Management\n",
    "\n",
    "In this section, we'll create test users and define their associated corpus access. This simulates a real-world scenario where different users have varying levels of data access.\n",
    "\n",
    "#### User Creation Process\n",
    "We'll create users in the Amazon Cognito user pool and store their unique identifiers for later use in access control. While this notebook uses a simplified user creation process for demonstration, in a production environment, you should follow your organization's best practices for user management and security.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> \n",
    "<br><b>Password minimum length:</b>8 character(s)\n",
    "<br><b>Password requirements</b>\n",
    "<br>Contains at least 1 number\n",
    "<br>Contains at least 1 special character\n",
    "<br>Contains at least 1 uppercase letter\n",
    "<br>Contains at least 1 lowercase letter\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7b1dc-08a2-4d89-972f-dddb1a3d0439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users = [\n",
    "    {\n",
    "        'name': 'Highway Harry',\n",
    "        'email': 'highway.harry@acmecorp.com',\n",
    "        'password': 'Highway.Harry.123$',\n",
    "        'corpus': ['highway']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Wildlife Walter',\n",
    "        'email': 'wildlife.walter@acmecorp.com',\n",
    "        'password': 'Wildlife.Walter.123$',\n",
    "        'corpus': ['wildlife']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Admin Amy',\n",
    "        'email': 'admin.amy@acmecorp.com',\n",
    "        'password': 'Admin.Amy.123$',\n",
    "        'corpus': ['highway', 'wildlife']\n",
    "    },\n",
    "]\n",
    "\n",
    "corpus = [\n",
    "    {\n",
    "        'name': 'highway',\n",
    "        'description': 'document regarding highway and roadsign regulations',\n",
    "        's3path': f's3://{0}/highway/'.format(bucket_name)\n",
    "    },\n",
    "    {\n",
    "        'name': 'wildlife',\n",
    "        'description': 'documents regarding fishing and hunting regulations',\n",
    "        's3path': f's3://{0}/wildlife/'.format(bucket_name)\n",
    "    },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0d5af-ecd1-40f1-b5a8-7413758915a3",
   "metadata": {},
   "source": [
    "#### Corpus-User Association\n",
    "For this example, we'll create users who have access to specific information and another user that has access to all content. This setup demonstrates how you can implement role-based access control in your knowledge base system.\n",
    "\n",
    "We use Universally Unique Identifiers (UUIDs) for corpus IDs to ensure uniqueness across our system. UUIDs are particularly useful in distributed systems where multiple parties might be generating identifiers simultaneously without coordination.\n",
    "\n",
    "Our user setup is as follows:\n",
    "\n",
    "1. Highway Harry: Access to highway-related documents only\n",
    "2. Wildlife Walter: Access to wildlife-related documents only\n",
    "3. Admin Amy: Access to both highway and wildlife documents\n",
    "\n",
    "This structure allows us to demonstrate how different access levels can be implemented and enforced within the same knowledge base.\n",
    "\n",
    "Each corpus (highway and wildlife) is associated with a unique UUID. When we create users, we'll associate them with these corpus IDs based on their access level. This association will later be used to filter Knowledge Base queries, ensuring users only access authorized information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131a29e-c8b0-4446-b00f-638f90b8ab98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_ids = []\n",
    "corpus_ids = []\n",
    "\n",
    "def create_user(user_data, user_type):\n",
    "    user_ids = []\n",
    "    for user in user_data:\n",
    "        response = cognito_client.admin_create_user(\n",
    "            UserPoolId=userpoolid,\n",
    "            Username=user['email'],\n",
    "            UserAttributes=[\n",
    "                {'Name': 'name', 'Value': user['name']},\n",
    "                {'Name': 'email', 'Value': user['email']},\n",
    "                {'Name': 'email_verified', 'Value': 'true'}\n",
    "            ],\n",
    "            ForceAliasCreation=False,\n",
    "            MessageAction='SUPPRESS'\n",
    "        )\n",
    "        cognito_client.admin_set_user_password(\n",
    "            UserPoolId=userpoolid,\n",
    "            Username=user['email'],\n",
    "            Password=user['password'],\n",
    "            Permanent=True\n",
    "        )\n",
    "        print(f\"{user_type.capitalize()} created:\", response['User']['Username'])\n",
    "        print(f\"{user_type.capitalize()} id:\", response['User']['Attributes'][3]['Value'])\n",
    "        user_ids.append(response['User']['Attributes'][3]['Value'])\n",
    "    return user_ids\n",
    "\n",
    "user_ids = create_user(users, 'user')\n",
    "corpus_ids = [str(uuid.uuid4()) for c in corpus]\n",
    "\n",
    "print(\"User IDs:\", user_ids)\n",
    "print(\"Corpus IDs:\", corpus_ids)\n",
    "\n",
    "%store user_ids corpus_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6879fe-131c-4469-b6c8-27038ecbbb66",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll implement this user-corpus association and create our test users in the Cognito User Pool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293facc-39be-46eb-a7e9-696a1084c679",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. User-Corpus Association in DynamoDB\n",
    "\n",
    "This section demonstrates how to populate the pre-created DynamoDB table with user-corpus associations. This table acts as a lookup for determining which corpus (or set of documents) a user has permission to access.\n",
    "\n",
    "#### Process Overview\n",
    "1. We'll use the DynamoDB resource to interact with our table.\n",
    "2. For each user, we'll create an entry in the table that lists the corpus IDs they have access to.\n",
    "3. This association will later be used to filter Knowledge Base queries, ensuring users only access authorized information.\n",
    "\n",
    "#### Implementation Details\n",
    "- The table schema uses the user's ID as the partition key.\n",
    "- The corpus IDs are stored as a list, allowing for multiple corpus associations per user.\n",
    "- This flexible structure allows for easy updates to user permissions by modifying their corpus list.\n",
    "\n",
    "\n",
    "#### Corpus-User Association\n",
    "For this example, we'll create users who have access to specific information and another user that has access to all content. This setup demonstrates how you can implement role-based access control in your knowledge base system.\n",
    "\n",
    "We use Universally Unique Identifiers (UUIDs) for corpus IDs to ensure uniqueness across our system. UUIDs are particularly useful in distributed systems where multiple parties might be generating identifiers simultaneously without coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15faadd4-960f-47ed-8793-b65fc2a65e28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table = dynamodb_resource.Table(dynamotable)\n",
    "corpus_mapping = [entry['name'] for entry in corpus]\n",
    "with table.batch_writer() as batch:\n",
    "    for corpus_list,user in enumerate(users):\n",
    "        temp = []\n",
    "        for corpus_id,corpuses in enumerate(corpus_mapping):\n",
    "            if corpuses in user['corpus']:\n",
    "                temp.append(corpus_ids[corpus_id])\n",
    "\n",
    "        batch.put_item(\n",
    "            Item={\n",
    "                'user_id': user_ids[corpus_list],\n",
    "                'corpus_id_list': temp\n",
    "            }\n",
    "        )\n",
    "\n",
    "print('Data inserted successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa27270-941e-482b-b5a9-6892d2d55362",
   "metadata": {},
   "source": [
    "### 3. Dataset Preparation and Upload\n",
    "\n",
    "In this section, we'll prepare the document corpus from the 'source_content' folder that will form the basis of our Knowledge Base.\n",
    "\n",
    "#### Process Overview\n",
    "1. The documents are organized into different folders, representing different data categories or access levels.\n",
    "2. We'll upload these documents to our own S3 bucket for use in the Knowledge Base.\n",
    "\n",
    "#### Implementation Details\n",
    "- The documents are stored in two different folders, simulating different data categories.\n",
    "- We use the boto3 S3 client to interact with the S3 buckets.\n",
    "- The `upload_file` method is used to transfer files from the source to our destination bucket.\n",
    "\n",
    "#### Considerations\n",
    "In a production environment, ensure that:\n",
    "  1. You have the necessary permissions to access and download the source documents.\n",
    "  2. The transfer process is secure, preferably using encryption in transit.\n",
    "  3. You implement error handling and logging for the file transfer process.\n",
    "  4. Consider implementing a mechanism to handle large datasets, such as multipart uploads or batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd310c44-6c3e-4c74-889b-43e9aa52abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists of URLs\n",
    "highway_urls = [\n",
    "    \"https://www.govinfo.gov/content/pkg/CFR-2024-title23-vol1/pdf/CFR-2024-title23-vol1-part655.pdf\",  \n",
    "    \"https://en.wikipedia.org/wiki/Federal_Highway_Administration\",\n",
    "    \"https://highways.dot.gov/sites/fhwa.dot.gov/files/2023-05/FHWA_Strategic_Plan_05.25.23.pdf\"\n",
    "]\n",
    "wildlife_urls = [\n",
    "    \"https://www.govinfo.gov/content/pkg/CFR-2023-title50-vol1/pdf/CFR-2023-title50-vol1-part13.pdf\",  \n",
    "    \"https://en.wikipedia.org/wiki/Loon\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f59f19-b6ac-4f47-93bf-bd1cfce59012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create folders if they don't exist and clear them if they do\n",
    "def create_folders():\n",
    "    base_folder = \"source_content\"\n",
    "    highway_folder = os.path.join(base_folder, \"highway\")\n",
    "    wildlife_folder = os.path.join(base_folder, \"wildlife\")\n",
    "\n",
    "    # Create the base folder if it doesn't exist\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    \n",
    "    # Clear contents of the highway folder\n",
    "    if os.path.exists(highway_folder):\n",
    "        for file in os.listdir(highway_folder):\n",
    "            file_path = os.path.join(highway_folder, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(highway_folder)\n",
    "    \n",
    "    # Clear contents of the wildlife folder\n",
    "    if os.path.exists(wildlife_folder):\n",
    "        for file in os.listdir(wildlife_folder):\n",
    "            file_path = os.path.join(wildlife_folder, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    else:\n",
    "        os.makedirs(wildlife_folder)\n",
    "\n",
    "    return highway_folder, wildlife_folder\n",
    "\n",
    "\n",
    "# Function to determine if a URL has a file extension or not\n",
    "def has_file_extension(url):\n",
    "    # Extract the filename from the URL path\n",
    "    filename = os.path.basename(url)\n",
    "    print(filename)\n",
    "    # Try to guess the file type using mimetypes\n",
    "    mime_type, _ = mimetypes.guess_type(filename)\n",
    "    return mime_type is not None\n",
    "\n",
    "# Function to download content from URLs and save to the appropriate folder\n",
    "def download_content(url_list, folder):\n",
    "    for i, url in enumerate(url_list, start=1):\n",
    "        try:\n",
    "            # Get the content from the URL\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # check for any request errors\n",
    "            \n",
    "            if has_file_extension(url):\n",
    "                # Handle as a binary file (e.g., .jpg, .pdf)\n",
    "                file_extension = os.path.splitext(url)[1]  # Extract file extension\n",
    "                filename = os.path.basename(url)\n",
    "                filename = os.path.join(folder, f\"{filename}\")\n",
    "                with open(filename, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"Downloaded file: {filename}\")\n",
    "            else:\n",
    "                # Handle as HTML\n",
    "                filename = os.path.basename(url)\n",
    "                filename = os.path.join(folder, f\"{filename}.html\")\n",
    "                with open(filename, \"w\", encoding='utf-8') as file:\n",
    "                    file.write(response.text)\n",
    "                print(f\"Downloaded HTML: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6658c47-5f1c-439d-8fbb-37d5434135f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files for each category\n",
    "highway_folder, wildlife_folder = create_folders()\n",
    "download_content(highway_urls, highway_folder)\n",
    "download_content(wildlife_urls, wildlife_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda19f76-ae71-4862-9f79-b1a5d7b2a2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "abs_path = os.path.abspath(\"source_content\")\n",
    "\n",
    "for root, dirs, files in os.walk(abs_path):\n",
    "    for file_name in files:\n",
    "        # Construct the full local path to the file\n",
    "        local_file_path = os.path.join(root, file_name)\n",
    "        \n",
    "        # Construct the S3 key (object key) using the relative path of the file\n",
    "        s3_key = os.path.relpath(local_file_path, abs_path)\n",
    "        \n",
    "        # Upload the file to S3\n",
    "        s3_client.upload_file(local_file_path, s3bucket, s3_key)\n",
    "        \n",
    "        print(f'{local_file_path} uploaded successfully to {s3bucket} with key {s3_key}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f55cd0-4093-411d-bab9-0ba2f0f6d88f",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "After uploading the documents, we'll associate metadata with each file to enable fine-grained access control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17575af2-f6e7-4225-a24e-411543a6976a",
   "metadata": {},
   "source": [
    "### 4. Metadata Association\n",
    "\n",
    "This crucial step involves attaching metadata to each document in our S3 bucket. This metadata will be used to implement access control at the document level.\n",
    "\n",
    "#### Process Overview\n",
    "1. We iterate through each corpus and its corresponding files in the S3 bucket.\n",
    "2. For each file, we create a metadata JSON file that includes a `corpus_id`.\n",
    "3. This metadata file is uploaded alongside the original document.\n",
    "\n",
    "#### Implementation Details\n",
    "- We use the `list_objects_v2` method to get all files in each corpus folder.\n",
    "- For each file, we create a metadata JSON structure with a `corpus_id`.\n",
    "- The metadata file is named `<original_filename>.metadata.json`.\n",
    "- We use the S3 `put_object` method to upload the metadata files.\n",
    "\n",
    "#### Importance of Metadata\n",
    "The `corpus_id` in the metadata allows us to:\n",
    "1. Associate documents with specific user access levels.\n",
    "2. Implement fine-grained filtering in our Knowledge Base queries.\n",
    "3. Ensure users only retrieve information they're authorized to access.\n",
    "\n",
    "#### Security Considerations\n",
    "- Ensure that metadata files are treated with the same level of security as the documents they describe.\n",
    "- Implement versioning on your S3 bucket to track changes to both documents and metadata.\n",
    "- Consider encrypting sensitive metadata information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8769a-b265-4cb9-bdb3-599ff4be305a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop through the corpus and their corresponding IDs\n",
    "for corpuses, corpus_entry in enumerate(corpus):\n",
    "    corpus_id = corpus_ids[corpuses]\n",
    "    s3path = corpus_entry['s3path']\n",
    "    \n",
    "    # Get bucket and prefix\n",
    "    # Remove 's3://' and split bucket and prefix\n",
    "    path_parts = s3path.replace('s3://', '').split('/', 1)\n",
    "    bucket = path_parts[0]\n",
    "    prefix = path_parts[1] if len(path_parts) > 1 else ''\n",
    "    \n",
    "    # List all files in the S3 folder\n",
    "    response = s3_client.list_objects_v2(Bucket=s3bucket, Prefix=prefix)\n",
    "    if 'Contents' in response:\n",
    "        files = [obj['Key'] for obj in response['Contents'] if obj['Key'] != prefix]\n",
    "    else:\n",
    "        files = []\n",
    "    \n",
    "    for file in files:\n",
    "        metadata = {\n",
    "            \"metadataAttributes\": {\n",
    "                \"corpus_id\": corpus_id\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Upload metadata file to S3\n",
    "        s3_client.put_object(\n",
    "            Bucket=s3bucket,\n",
    "            Key=f\"{file}.metadata.json\",\n",
    "            Body=json.dumps(metadata, indent=4),\n",
    "            ContentType='application/json'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819b55c-3321-4859-ae92-8c35dd16c16a",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "With our documents and metadata in place, we'll proceed to set up the OpenSearch Serverless collection that will power our Knowledge Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700ad37-73b8-4200-a7da-efe66ae02f1f",
   "metadata": {},
   "source": [
    "### 5. Create OpenSearch Serverless Collection\n",
    "\n",
    "In this section, we'll create an OpenSearch Serverless collection to be used by our Amazon Bedrock Knowledge Base. OpenSearch Serverless provides a fully managed search and analytics service that's easy to set up and scale.\n",
    "\n",
    "We're using OpenSearch Serverless for this knowledge base implementation because it offers several advantages:\n",
    "1. Automatic scaling based on workload\n",
    "2. Pay-only-for-what-you-use pricing model\n",
    "3. Simplified operations with no clusters to manage\n",
    "4. Built-in security features\n",
    "\n",
    "#### Process Overview\n",
    "1. Create an encryption policy for the collection.\n",
    "2. Create a network policy to control access to the collection.\n",
    "3. Create a data access policy to manage permissions.\n",
    "4. Create the OpenSearch Serverless collection itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b862f80-f831-4e10-b754-262e8caaa76f",
   "metadata": {},
   "source": [
    "#### Step 1: Create Encryption Policy\n",
    "We start by creating an encryption policy that specifies how data in our collection should be encrypted. This uses AWS-owned keys for simplicity, but in a production environment, you might consider using customer-managed keys for additional control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489ef438-e4da-4657-a1e3-8a4de72b36ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = '{{\"Rules\":[{{\"ResourceType\": \"collection\", \"Resource\":[\"collection/namer-{0}-kbcollection\"]}}], \"AWSOwnedKey\": true}}'.format(account_id)\n",
    "print(policy)\n",
    "\n",
    "results = opensearch.create_security_policy(\n",
    "    description='Public encryption access namer workshop collection',\n",
    "    name='namer-' + account_id + '-kbenc',\n",
    "    policy=policy,\n",
    "    type='encryption'\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7e4ec-ad37-4cd1-a693-a21e30f1f148",
   "metadata": {},
   "source": [
    "#### Step 2: Create Network Policy\n",
    "Next, we create a network policy that defines how the collection can be accessed. For this demo, we're allowing public access, but in a production environment, you'd want to restrict this to specific VPCs or IP ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf22ad-37ef-4c0f-ac9c-5a2f8070022f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = '''[{{\"Rules\": [{{\"ResourceType\": \"dashboard\", \n",
    "    \"Resource\": [\"collection/namer-{0}-kbcollection\"]}}, \n",
    "    {{\"ResourceType\": \"collection\", \"Resource\": [\"collection/namer-{0}-kbcollection\"]}}], \n",
    "    \"AllowFromPublic\": true}}]'''.format(account_id)\n",
    "print(policy)\n",
    "results = opensearch.create_security_policy(\n",
    "    description='Public network access namer workshop collection',\n",
    "    name='namer-{0}-kbnet'.format(account_id),\n",
    "    policy=policy,\n",
    "    type='network'\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d826c-ab50-4413-8d78-56c1b285f83c",
   "metadata": {},
   "source": [
    "#### Step 3: Create Data Access Policy\n",
    "The data access policy determines who can perform what actions on the collection. We're granting broad permissions to our IAM role for this demo, but in practice, you'd want to follow the principle of least privilege."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a89b4c-61c3-4ef3-9ac1-de2ccd43d6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = '''[{{\"Rules\": [{{\"Resource\": [\"collection/namer-{0}-kbcollection\"], \n",
    "                           \"Permission\": [\"aoss:CreateCollectionItems\", \"aoss:UpdateCollectionItems\", \"aoss:DescribeCollectionItems\"], \n",
    "                           \"ResourceType\": \"collection\"}}, \n",
    "                          {{\"ResourceType\": \"index\", \"Resource\": [\"index/namer-{0}-kbcollection/*\"], \n",
    "                           \"Permission\": [\"aoss:CreateIndex\", \"aoss:DescribeIndex\", \"aoss:ReadDocument\", \"aoss:WriteDocument\", \"aoss:UpdateIndex\", \"aoss:DeleteIndex\"]}}], \n",
    "                \"Principal\": [\"arn:aws:iam::{0}:role/Namer-{0}-KBRole\"]}}]'''.format(account_id)\n",
    "print(policy)\n",
    "results = opensearch.create_access_policy(\n",
    "    description='Data access policy for the NAMER summit',\n",
    "    name='namer-{0}-kbaccess'.format(account_id),\n",
    "    policy=policy,\n",
    "    type='data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66443aa1-802a-406c-b58d-9bbb4c2cf4c9",
   "metadata": {},
   "source": [
    "Now that we have our policies we can create the OpenSearch Serverless Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85fe8a-1e95-4134-9cbc-7be74a60f2fb",
   "metadata": {},
   "source": [
    "#### Step 4: Create OpenSearch Serverless Collection\n",
    "Finally, we create the actual OpenSearch Serverless collection, applying the policies we've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4e8f0-2b25-4de2-b947-1ced00c3a0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = opensearch.create_collection(\n",
    "    description='KB AOSS Collection',\n",
    "    name='namer-{0}-kbcollection'.format(account_id),\n",
    "    type='VECTORSEARCH'\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f7a64-933f-4e80-a722-98c0680e5bae",
   "metadata": {},
   "source": [
    "Creating the collection takes some time so we will check to see if it has been created yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c342988-f55c-4628-a0f1-e73cc9ed82f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = opensearch.list_collections(collectionFilters={'name':'namer-{0}-kbcollection'.format(account_id)})\n",
    "collection_id = response[\"collectionSummaries\"][0][\"id\"]\n",
    "collection_arn = response[\"collectionSummaries\"][0][\"arn\"]\n",
    "print(\"Creating OpenSearch Collection\")\n",
    "while opensearch.list_collections(collectionFilters={'name':'namer-{0}-kbcollection'.format(account_id)})[\"collectionSummaries\"][0][\"status\"] != \"ACTIVE\":\n",
    "    time.sleep(30)\n",
    "    print(\"in progress....\")\n",
    "\n",
    "print(\"Collection created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b62c6a-08e1-48c1-9432-e230d0df30ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexName = \"namer-{0}-kb-acl-index\".format(account_id)\n",
    "print(\"Index name:\",indexName)\n",
    "%store indexName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a583a886-dbdd-4edd-abd9-246ebcda02cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the current role to the collection's data access policy\n",
    "data_access_policy_name = 'namer-{0}-kbaccess'.format(account_id)\n",
    "\n",
    "response = opensearch.get_access_policy(\n",
    "    name=data_access_policy_name,\n",
    "    type='data'\n",
    ")\n",
    "policy_version = response[\"accessPolicyDetail\"][\"policyVersion\"]\n",
    "existing_policy = response['accessPolicyDetail']['policy']\n",
    "updated_policy = existing_policy.copy()\n",
    "updated_policy[0]['Principal'].append('arn:aws:iam::{0}:role/{1}-{0}-SageMaker-Execution-Namer-2024-Role'.format(account_id, boto3.session.Session().region_name))\n",
    "updated_policy = str(updated_policy).replace(\"'\", '\"')\n",
    "\n",
    "response = opensearch.update_access_policy(\n",
    "    description='dataAccessPolicy',\n",
    "    name=data_access_policy_name,\n",
    "    policy=updated_policy,\n",
    "    policyVersion=policy_version,\n",
    "    type='data'\n",
    ")\n",
    "print(response)\n",
    "\n",
    "time.sleep(60) # Changes to the data access policy might take a bit to update\n",
    "print(\"Finished adding the role\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0b5e4-1e5c-4659-9192-5fe4dbc844f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up AWS authentication\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "# Define index settings and mappings\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": \"true\"\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                 \"method\": {\n",
    "                     \"name\": \"hnsw\",\n",
    "                     \"engine\": \"faiss\",\n",
    "                     \"space_type\": \"innerproduct\",\n",
    "                     \"parameters\": {\n",
    "                         \"ef_construction\": 512,\n",
    "                         \"m\": 16\n",
    "                     },\n",
    "                 },\n",
    "             },\n",
    "            \"text\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"text-metadata\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Build the OpenSearch client\n",
    "host = f\"{collection_id}.{region}.aoss.amazonaws.com\"\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "# Create index\n",
    "response = oss_client.indices.create(index=indexName, body=json.dumps(index_settings))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791e6e23-8787-4d28-88e4-f423382ba1c9",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "With our OpenSearch Serverless collection in place, we're ready to create and configure our Knowledge Base in Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311ef05-a4f4-40fd-9ac5-8f3d678b89df",
   "metadata": {},
   "source": [
    "### 6. Create and Configure Knowledge Base for Amazon Bedrock\n",
    "\n",
    "This section covers the process of creating and configuring a Knowledge Base using Amazon Bedrock. This Knowledge Base will utilize the OpenSearch Serverless collection we just created to store and retrieve document embeddings.\n",
    "\n",
    "#### Process Overview\n",
    "1. Select an embedding model for the Knowledge Base.\n",
    "2. Create the Knowledge Base using Amazon Bedrock.\n",
    "3. Create a data source linking to our S3 bucket.\n",
    "4. Start an ingestion job to populate the Knowledge Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4825b-4127-4191-b438-df1ffa55721a",
   "metadata": {},
   "source": [
    "#### Step 1: Select Embedding Model\n",
    "We'll use Amazon Titan Embeddings V2 for this Knowledge Base. This model will convert our text documents into vector embeddings for efficient similarity search.\n",
    "\n",
    "Important: Ensure you have enabled access to Amazon Titan Embeddings V2 in the Amazon Bedrock Console before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470be25-3c79-4eb0-b3bf-c419c1878a52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddingModelArn = \"arn:aws:bedrock:{}::foundation-model/amazon.titan-embed-text-v2:0\".format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f5d386-bd9b-4d0e-b660-4fb347ad07fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Create Knowledge Base\n",
    "We use the `create_knowledge_base` API call to set up our Knowledge Base. Key configuration points include:\n",
    "- Specifying the embedding model.\n",
    "- Linking to our OpenSearch Serverless collection.\n",
    "- Setting up field mappings for text, metadata, and vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951db5fd-d6ca-48f8-8803-4cbd3b50bf2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = bedrock_agent_client.create_knowledge_base(\n",
    "    description='Test KB Deployment',\n",
    "    knowledgeBaseConfiguration={\n",
    "        'type': 'VECTOR',\n",
    "        'vectorKnowledgeBaseConfiguration': {\n",
    "            'embeddingModelArn': embeddingModelArn\n",
    "        }\n",
    "    },\n",
    "    name='namer-{0}-knowledge-base'.format(account_id),\n",
    "    roleArn='arn:aws:iam::{0}:role/Namer-{0}-KBRole'.format(account_id),\n",
    "    storageConfiguration={\n",
    "        'opensearchServerlessConfiguration': {\n",
    "            'collectionArn': collection_arn,\n",
    "            'fieldMapping': {\n",
    "                'metadataField': 'text-metadata',\n",
    "                'textField': 'text',\n",
    "                'vectorField': 'vector'\n",
    "            },\n",
    "            'vectorIndexName': indexName\n",
    "        },\n",
    "        'type': 'OPENSEARCH_SERVERLESS'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "kb_id = results[\"knowledgeBase\"][\"knowledgeBaseId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e1a5c-952a-4649-8015-c41e2bfcc2e6",
   "metadata": {},
   "source": [
    "#### Step 3: Create Data Source\n",
    "We create a data source that points to our S3 bucket containing the documents and metadata. This step uses the `create_data_source` API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b961dae2-75b8-4814-a1b9-0f1937834f10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = bedrock_agent_client.create_data_source(\n",
    "    dataSourceConfiguration={\n",
    "        's3Configuration': {\n",
    "            'bucketArn': 'arn:aws:s3:::{0}'.format(s3bucket),\n",
    "        },\n",
    "        'type': 'S3',\n",
    "    },\n",
    "    description='KB Data Source',\n",
    "    knowledgeBaseId=kb_id,\n",
    "    name='namer-{0}-kb_datasource'.format(account_id),\n",
    "\n",
    "    vectorIngestionConfiguration={\n",
    "        'chunkingConfiguration': {\n",
    "            'chunkingStrategy': 'FIXED_SIZE',\n",
    "            'fixedSizeChunkingConfiguration': {\n",
    "                'maxTokens': 300,\n",
    "                'overlapPercentage': 20\n",
    "            },\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(results)\n",
    "\n",
    "datasource_id = results[\"dataSource\"][\"dataSourceId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d13750-272a-4e0b-93df-060b93e50171",
   "metadata": {},
   "source": [
    "#### Step 4: Start Ingestion Job\n",
    "Finally, we initiate an ingestion job to process our documents, generate embeddings, and store them in the OpenSearch collection. We monitor the job status to ensure successful completion.\n",
    "\n",
    "#### Important Considerations\n",
    "- The ingestion process can take some time, depending on the volume of documents.\n",
    "- Ensure your IAM roles have the necessary permissions for all these operations.\n",
    "- In a production environment, consider setting up monitoring and alerting for ingestion jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9c800-6e86-4fee-b09a-b8f4777d7fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ingestion_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "    knowledgeBaseId=kb_id,\n",
    "    dataSourceId=datasource_id,\n",
    "    description='Initial Ingestion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e2bbf-6a21-4cfd-95e4-2102fdcb3b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status = bedrock_agent_client.get_ingestion_job(\n",
    "    knowledgeBaseId=ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "    dataSourceId=ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "    ingestionJobId=ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    ")[\"ingestionJob\"][\"status\"]\n",
    "print(status)\n",
    "while status not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "    status = bedrock_agent_client.get_ingestion_job(\n",
    "        knowledgeBaseId=ingestion_job_response[\"ingestionJob\"][\"knowledgeBaseId\"],\n",
    "        dataSourceId=ingestion_job_response[\"ingestionJob\"][\"dataSourceId\"],\n",
    "        ingestionJobId=ingestion_job_response[\"ingestionJob\"][\"ingestionJobId\"]\n",
    "    )[\"ingestionJob\"][\"status\"]\n",
    "    print(status)\n",
    "    time.sleep(30)\n",
    "print(\"Waiting for changes to take place in the vector database\")\n",
    "time.sleep(30) # Wait for all changes to take place\n",
    "print(\"COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b18f36-41ec-4083-a4f9-a08fc5ed3144",
   "metadata": {},
   "source": [
    "#### Testing the Knowledge Base\n",
    "After ingestion is complete, we demonstrate how to use the `retrieve` and `retrieve_and_generate` APIs to query our Knowledge Base. These examples show how to:\n",
    "- Filter queries based on corpus IDs (implementing our access control).\n",
    "- Retrieve relevant document chunks.\n",
    "- Generate responses using a language model based on the retrieved information.\n",
    "\n",
    "The `retrieve` API is used when you only need to fetch relevant passages from the Knowledge Base. It's useful when you want to see the raw, unprocessed information or when you plan to process the retrieved information yourself.\n",
    "\n",
    "The `retrieve_and_generate` API goes a step further. It not only retrieves relevant passages but also uses a specified language model to generate a coherent response based on those passages. This is particularly useful when you want to provide a more human-friendly, synthesized answer to a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b971c-e879-4d81-81c1-365f052523f3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> Make sure you have enabled Anthropic Claude 3 Sonnet access in the Amazon Bedrock Console (model access). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6452bb02-651f-4d3f-8480-6eddb4b7f898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve and generate API\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": \"Which office do I submit for golden eagle permits?\"\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\".format(region),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5,\n",
    "                    \"filter\": {\n",
    "                        \"equals\": {\n",
    "                            \"key\": \"corpus_id\",\n",
    "                            \"value\": corpus_ids[1]\n",
    "                        }\n",
    "                    }\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc2f8dd-aae4-4461-aa65-0f82cc74abc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this second example we are going to use the **retrieve API**. This API queries the knowledge base and retrieves relavant information from it, it does not generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb520b34-f6c9-4caa-bd00-3c8d65c9745a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":3,\n",
    "            \"filter\": {\n",
    "                 \"equals\": {\n",
    "                    \"key\": \"corpus_id\",\n",
    "                    \"value\": corpus_ids[1]\n",
    "                        }\n",
    "                    }\n",
    "                } \n",
    "            },\n",
    "    retrievalQuery={\n",
    "        'text': \"Which office do I submit for golden eagle permits?\"   \n",
    "        }\n",
    ")\n",
    "\n",
    "def response_print(retrieve_resp):\n",
    "#structure 'retrievalResults': list of contents\n",
    "# each list has content,location,score,metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n",
    "\n",
    "response_print(response_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b222b-47e4-4765-932f-7b9fbf2f8e0d",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "With our Knowledge Base set up and populated, we'll move on to updating our Lambda function to use the latest SDK, ensuring compatibility with our metadata filtering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad65e10-3bad-4ca9-8009-8f85ed91f92e",
   "metadata": {},
   "source": [
    "### 7. Update AWS Lambda Function\n",
    "\n",
    "At the time of developing this notebook, the default Boto3 version available in Lambda with Python 3.12 doesn't include metadata filtering capabilities for Bedrock Knowledge Bases. To overcome this limitation, we'll create and attach an AWS Lambda Layer with the latest Boto3 version.\n",
    "\n",
    "Updating the Lambda layer is necessary to ensure that our function has access to the latest AWS SDK features, particularly those related to Bedrock Knowledge Bases and metadata filtering. \n",
    "\n",
    "#### Process Overview\n",
    "1. Create a directory for the Lambda layer.\n",
    "2. Install the latest Boto3 and Botocore versions in this directory.\n",
    "3. Create a ZIP file of the installed packages.\n",
    "4. Publish a new Lambda layer using this ZIP file.\n",
    "5. Attach the new layer to our existing Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c3644-09c3-4b11-aac9-2732204572f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# can we have the lambda layer already attached to the lambda function?  What is we have it prebuilt?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36088ee6-8c52-4bea-8926-ac7a8d55fde7",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "This section requires the `zip` package to be installed at the system level. You can check if it's installed by running the `!zip` command. If it's not available, you'll need to install it using the appropriate package manager for your system (e.g., `apt-get` for Debian-based systems or `yum` for RHEL-based systems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2126d9-7fe1-421c-879e-447ca4a1915d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir latest-sdk-layer\n",
    "%cd latest-sdk-layer\n",
    "!pip install -qU boto3 botocore -t python/lib/python3.12/site-packages/\n",
    "!zip -rq latest-sdk-layer.zip .\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4894d13-7b90-40fc-96d2-de517574ff49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def publish_lambda_layer(layer_name, description, zip_file_path, compatible_runtimes):\n",
    "    with open(zip_file_path, 'rb') as f:\n",
    "        response = lambda_client.publish_layer_version(\n",
    "            LayerName=layer_name,\n",
    "            Description=description,\n",
    "            Content={\n",
    "                'ZipFile': f.read(),\n",
    "            },\n",
    "            CompatibleRuntimes=compatible_runtimes\n",
    "        )\n",
    "    return response['LayerVersionArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171637fa-5671-44a3-be06-1f3400ec8fa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_name = 'latest-sdk-layer'\n",
    "description = 'Layer with the latest boto3 version.'\n",
    "zip_file_path = 'latest-sdk-layer/latest-sdk-layer.zip'\n",
    "compatible_runtimes = ['python3.12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf2e5c7-a29a-4d20-bd49-c6df4b605a80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_version_arn = publish_lambda_layer(layer_name, description, zip_file_path, compatible_runtimes)\n",
    "print(\"Layer version ARN:\", layer_version_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efea5300-419a-4641-b4e8-453f434220a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Add the layer to the Lambda function\n",
    "    lambda_client.update_function_configuration(\n",
    "        FunctionName=lambdafunctionarn,\n",
    "        Layers=[layer_version_arn]\n",
    "    )\n",
    "    print(\"Layer added to the Lambda function successfully.\")\n",
    "\n",
    "except ClientError as e:\n",
    "    print(f\"Error adding layer to Lambda function: {e.response['Error']['Message']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbaed0e-3ee5-4597-902c-f7e1cb054dbb",
   "metadata": {},
   "source": [
    "add function to the VPC and update the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598aa8d3-1f15-4480-82bb-d45f37720060",
   "metadata": {},
   "source": [
    "add \n",
    "```    \n",
    "    {\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Action\": [\n",
    "                \"bedrock:RetrieveAndGenerate\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"ForAnyValue:StringEquals\": {\n",
    "                    \"aws:sourceVpce\": [\n",
    "                        \"vpce-0f88bae3249d891d9\" <vpce id of the bedrock-agent-runtime>\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "```\n",
    "    to lambda exectution role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89dc9e-61dc-4134-957a-cd13915f8f47",
   "metadata": {},
   "source": [
    "#### Next Steps\n",
    "With our Lambda function now using the latest Boto3 version, we're ready to create a user interface to interact with our metadata-filtered Knowledge Base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db5760a-f8e6-4ec6-852a-273afa467dfe",
   "metadata": {},
   "source": [
    "### 8. Create and Run Streamlit Application\n",
    "\n",
    "To showcase the interaction between users and our Knowledge Base with metadata filtering, we'll create a simple web application using Streamlit. This popular Python library allows us to quickly build interactive data apps.\n",
    "\n",
    "#### Application Overview\n",
    "The Streamlit app will:\n",
    "1. Authenticate users using Amazon Cognito.\n",
    "2. Retrieve the user's associated corpus IDs from DynamoDB.\n",
    "3. Allow users to enter search queries.\n",
    "4. Use our Lambda function to query the Knowledge Base with appropriate metadata filters.\n",
    "5. Display the results to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b177ac-18c6-48e7-bc3c-e840f88b17a2",
   "metadata": {},
   "source": [
    "#### Implementation Steps\n",
    "1. We define the Streamlit app in a Python file (`app.py`).\n",
    "2. The app uses the `streamlit_cognito_auth` package for user authentication.\n",
    "3. After authentication, it retrieves the user's Cognito sub (unique identifier) and associated corpus IDs.\n",
    "4. The search functionality invokes our Lambda function, passing the user's query and corpus IDs for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a1b06-6fb0-4c36-a0b0-27319413d8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import streamlit as st\n",
    "from streamlit_cognito_auth import CognitoAuthenticator\n",
    "\n",
    "pool_id = \"<<replace_pool_id>>\"\n",
    "app_client_id = \"<<replace_app_client_id>>\"\n",
    "app_client_secret = \"<<replace_app_client_secret>>\"\n",
    "kb_id = \"<<replace_kb_id>>\"\n",
    "lambda_function_arn = '<<replace_lambda_function_arn>>'\n",
    "dynamo_table = '<<replace_dynamo_table_name>>'\n",
    "\n",
    "authenticator = CognitoAuthenticator(\n",
    "    pool_id=pool_id,\n",
    "    app_client_id=app_client_id,\n",
    "    app_client_secret= app_client_secret,\n",
    "    use_cookies=False\n",
    ")\n",
    "\n",
    "is_logged_in = authenticator.login()\n",
    "\n",
    "if not is_logged_in:\n",
    "    st.stop()\n",
    "\n",
    "def logout():\n",
    "    authenticator.logout()\n",
    "\n",
    "def get_user_sub(userpoolid, username):\n",
    "    cognito_client = boto3.client('cognito-idp')\n",
    "    try:\n",
    "        response = cognito_client.admin_get_user(\n",
    "            UserPoolId=pool_id,\n",
    "            Username=authenticator.get_username()\n",
    "        )\n",
    "        sub = None\n",
    "        for attr in response['UserAttributes']:\n",
    "            if attr['Name'] == 'sub':\n",
    "                sub = attr['Value']\n",
    "                break\n",
    "        return sub\n",
    "    except cognito_client.exceptions.UserNotFoundException:\n",
    "        print(\"User not found.\")\n",
    "        return None\n",
    "\n",
    "def get_corpus_ids(user_id):\n",
    "    dynamodb = boto3.client('dynamodb')\n",
    "    response = dynamodb.query(\n",
    "        TableName=dynamo_table,\n",
    "        KeyConditionExpression='user_id = :user_id',\n",
    "        ExpressionAttributeValues={\n",
    "            ':user_id': {'S': user_id}\n",
    "        }\n",
    "    )\n",
    "    print(response)\n",
    "    corpus_id_list = []  # Initialize the list\n",
    "    for item in response['Items']:\n",
    "        corpus_ids = item.get('corpus_id_list', {}).get('L', [])\n",
    "        corpus_id_list.extend([corpus_id['S'] for corpus_id in corpus_ids])\n",
    "    return corpus_id_list\n",
    "\n",
    "def search_transcript(user_id, kb_id, text, corpus_ids):\n",
    "    # Initialize the Lambda client\n",
    "    lambda_client = boto3.client('lambda')\n",
    "\n",
    "    # Payload for the Lambda function\n",
    "    payload = json.dumps({\n",
    "        \"userId\": sub,\n",
    "        \"knowledgeBaseId\": kb_id,\n",
    "        \"text\": text, \n",
    "        \"corpusIds\": corpus_ids\n",
    "    }).encode('utf-8')\n",
    "\n",
    "    try:\n",
    "        # Invoke the Lambda function\n",
    "        response = lambda_client.invoke(\n",
    "            FunctionName=lambda_function_arn,\n",
    "            InvocationType='RequestResponse',\n",
    "            Payload=payload\n",
    "        )\n",
    "\n",
    "        # Process the response\n",
    "        if response['StatusCode'] == 200:\n",
    "            response_payload = json.loads(response['Payload'].read().decode('utf-8'))\n",
    "            return response_payload\n",
    "        else:\n",
    "            # Handle error response\n",
    "            return {'error': 'Failed to fetch data'}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exception\n",
    "        return {'error': str(e)}\n",
    "\n",
    "sub = get_user_sub(pool_id, authenticator.get_username())\n",
    "print(sub)\n",
    "corpus_ids = get_corpus_ids(sub)\n",
    "print(corpus_ids)\n",
    "\n",
    "# Application Front\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"User Information\")\n",
    "    st.markdown(\"## User\")\n",
    "    st.text(authenticator.get_username())\n",
    "    st.markdown(\"## User Id\")\n",
    "    st.text(sub)\n",
    "    # selected_patient = st.selectbox(\"Select a patient (or 'All' for all patients)\", ['All'] + patient_ids)\n",
    "    st.button(\"Logout\", \"logout_btn\", on_click=logout)\n",
    "\n",
    "st.header(\"Corpus Search Tool\")\n",
    "\n",
    "# Text input for the search query\n",
    "query = st.text_input(\"Enter your search query:\")\n",
    "\n",
    "if st.button(\"Search\"):\n",
    "    if query:\n",
    "        # Perform search\n",
    "        corpus_ids_filter = corpus_ids\n",
    "        results = search_transcript(sub, kb_id, query, corpus_ids_filter)\n",
    "        print(results)\n",
    "        if results:\n",
    "            st.subheader(\"Search Results:\")\n",
    "            st.markdown(results[\"body\"], unsafe_allow_html=True)\n",
    "        else:\n",
    "            st.write(\"No matching results found in corpus.\")\n",
    "    else:\n",
    "        st.write(\"Please enter a search query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b58f02-bfa7-4c8f-80cb-f47c10ad1c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "replace_vars(\"app.py\", userpoolid, clientid, clientsecret, kb_id, lambdafunctionarn, dynamotable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f95a61b-f391-4bfb-a97a-064bfc09115c",
   "metadata": {},
   "source": [
    "#### Running the Application\n",
    "To run the Streamlit app:\n",
    "1. Execute the provided command in your notebook environment.\n",
    "2. Access the app using the URL provided, which will vary depending on your environment (SageMaker Studio or SageMaker Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a6b85-af35-4283-95fb-eae1c62e9f20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.18.10:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.236.55.223:8501\u001b[0m\n",
      "\u001b[0m\n",
      "b41804f8-6021-7036-64d6-ab5cb55380ae\n",
      "{'Items': [{'corpus_id_list': {'L': [{'S': 'd445ea2e-ff16-4e8c-a1a3-ae3a3011cd19'}]}, 'user_id': {'S': 'b41804f8-6021-7036-64d6-ab5cb55380ae'}}], 'Count': 1, 'ScannedCount': 1, 'ResponseMetadata': {'RequestId': '43G02OCUTM8SBGBKRE02VQBPAJVV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Thu, 12 Sep 2024 13:54:05 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '165', 'connection': 'keep-alive', 'x-amzn-requestid': '43G02OCUTM8SBGBKRE02VQBPAJVV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '1284113190'}, 'RetryAttempts': 0}}\n",
      "['d445ea2e-ff16-4e8c-a1a3-ae3a3011cd19']\n",
      "b41804f8-6021-7036-64d6-ab5cb55380ae\n",
      "{'Items': [{'corpus_id_list': {'L': [{'S': 'd445ea2e-ff16-4e8c-a1a3-ae3a3011cd19'}]}, 'user_id': {'S': 'b41804f8-6021-7036-64d6-ab5cb55380ae'}}], 'Count': 1, 'ScannedCount': 1, 'ResponseMetadata': {'RequestId': 'D8AHE3SP60TQ3FDGK6393CL06RVV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': {'server': 'Server', 'date': 'Thu, 12 Sep 2024 13:54:12 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '165', 'connection': 'keep-alive', 'x-amzn-requestid': 'D8AHE3SP60TQ3FDGK6393CL06RVV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '1284113190'}, 'RetryAttempts': 0}}\n",
      "['d445ea2e-ff16-4e8c-a1a3-ae3a3011cd19']\n",
      "{'errorMessage': 'An error occurred (AccessDeniedException) when calling the RetrieveAndGenerate operation: User: arn:aws:sts::850754977538:assumed-role/Namer-LambdaExecutionRole-1Ovdk1a9K7g7/namer-850754977538-lambda-function is not authorized to perform: bedrock:RetrieveAndGenerate on resource: arn:aws:bedrock:us-east-1:850754977538:knowledge-base/LL85IR1HOW because no VPC endpoint policy allows the bedrock:RetrieveAndGenerate action', 'errorType': 'AccessDeniedException', 'requestId': 'deb69a68-62d0-42f0-864c-fd3fdcaffd16', 'stackTrace': ['  File \"/var/task/index.py\", line 45, in lambda_handler\\n    response = bedrock_agent.retrieve_and_generate(\\n', '  File \"/opt/python/lib/python3.12/site-packages/botocore/client.py\", line 569, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n', '  File \"/opt/python/lib/python3.12/site-packages/botocore/client.py\", line 1023, in _make_api_call\\n    raise error_class(parsed_response, operation_name)\\n']}\n",
      "2024-09-12 13:54:16.703 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n",
      "    result = func()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 590, in code_to_exec\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/home/ec2-user/SageMaker/namer-summit-2024-genAI-privacy/app.py\", line 124, in <module>\n",
      "    st.markdown(results[\"body\"], unsafe_allow_html=True)\n",
      "KeyError: 'body'\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd734a82-46cf-48fc-b7fc-a28532b6fd5c",
   "metadata": {},
   "source": [
    "#### Important Notes\n",
    "- Use the email and password of the users you defined earlier in the notebook to log in to the application.\n",
    "- Once logged in, you can query the Knowledge Base. The results will be filtered based on your user's permissions.\n",
    "\n",
    "If you are executing this notebook on SageMaker Studio you can access the Streamlit application in the following url. \n",
    "\n",
    "```\n",
    "https://<<STUDIOID>>.studio.<<REGION>>.sagemaker.aws/jupyterlab/default/proxy/8501/\n",
    "```\n",
    "\n",
    "If you are executing this notebook on a SageMaker Notebook you can access the Streamlit application in the following url. \n",
    "\n",
    "```\n",
    "https://<<NOTEBOOKID>>.notebook.<<REGION>>.sagemaker.aws/proxy/8501/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1802e91-1826-4399-a3c2-34b787887c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://us-west-2-431615879134-sagemaker-execution-namer-2024-notebook.notebook.us-west-2.sagemaker.aws/proxy/8501/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8af412-654c-47da-8ebe-0e779261f161",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Next Steps\n",
    "After testing the application, proceed to the clean-up section to remove all created resources and avoid unnecessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc24e9-7516-4b6e-a6f4-2828784076f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. Clean-up Resources\n",
    "\n",
    "This final section guides you through the process of deleting all the resources created during this workshop. This step is crucial to avoid incurring unnecessary AWS costs.\n",
    "\n",
    "#### Important: Before proceeding, ensure you have stopped the Streamlit application running in the previous step.\n",
    "\n",
    "#### Clean-up Process\n",
    "1. Delete all objects in the S3 bucket.\n",
    "2. Delete the CloudFormation stacks that were created.\n",
    "\n",
    "#### Implementation Details\n",
    "- We use the S3 client to list and delete all objects in our bucket.\n",
    "- We iterate through our CloudFormation stacks, checking their status and deleting them if they exist.\n",
    "- The script waits for each stack deletion to complete before moving to the next one.\n",
    "\n",
    "#### Execution Time\n",
    "The clean-up process typically takes about 2-3 minutes to complete.\n",
    "\n",
    "#### Important Considerations\n",
    "- Double-check that all resources have been properly deleted by reviewing the AWS Console.\n",
    "- In a production environment, consider implementing a more robust clean-up process, possibly as part of your infrastructure-as-code setup.\n",
    "- Ensure that you have the necessary permissions to delete all created resources.\n",
    "\n",
    "#### Final Notes\n",
    "- Always be cautious when deleting resources. Ensure you're in the correct AWS account and region.\n",
    "- If you plan to recreate this setup later, consider saving key configuration details or automating the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ccc8b-a828-416f-9c93-9482c929f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete all objects in the S3 bucket\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=s3bucket)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_client.delete_object(Bucket=s3bucket, Key=obj['Key'])\n",
    "        print(f\"All objects in {s3bucket} have been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting objects from {s3bucket}: {e}\")\n",
    "\n",
    "# Delete the Knowledge Base\n",
    "try:\n",
    "    bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb_id)\n",
    "    print(f\"Knowledge Base {kb_id} deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting Knowledge Base: {e}\")\n",
    "\n",
    "# Delete the OpenSearch Serverless collection\n",
    "try:\n",
    "    opensearch.delete_collection(id=collection_id)\n",
    "    print(f\"OpenSearch Serverless collection {collection_id} deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting OpenSearch Serverless collection: {e}\")\n",
    "\n",
    "# Delete the OpenSearch Serverless security policies\n",
    "policy_names = [f'namer-{account_id}-kbenc', f'namer-{account_id}-kbnet']\n",
    "policy_types = ['encryption', 'network']\n",
    "\n",
    "for name, types in zip(policy_names, policy_types):\n",
    "    try:\n",
    "        opensearch.delete_security_policy(name=name, type=types)\n",
    "        print(f\"OpenSearch Serverless {types} policy {name} deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting OpenSearch Serverless {types} policy {name}: {e}\")\n",
    "        \n",
    "try:\n",
    "    opensearch.delete_access_policy(name=data_access_policy_name, type='data')\n",
    "    print(f\"OpenSearch Serverless data policy {data_access_policy_name} deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting OpenSearch Serverless data policy {data_access_policy_name}: {e}\")\n",
    "\n",
    "# Delete the Lambda layer\n",
    "try:\n",
    "    lambda_client.delete_layer_version(LayerName=layer_name, VersionNumber=int(layer_version_arn.split(':')[-1]))\n",
    "    print(f\"Lambda layer {layer_name} deleted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting Lambda layer: {e}\")\n",
    "\n",
    "# Delete users from Cognito User Pool\n",
    "for user in users:\n",
    "    try:\n",
    "        cognito_client.admin_delete_user(\n",
    "            UserPoolId=userpoolid,\n",
    "            Username=user['email']\n",
    "        )\n",
    "        print(f\"User {user['email']} deleted from Cognito User Pool.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting user {user['email']} from Cognito User Pool: {e}\")\n",
    "\n",
    "# Delete items from DynamoDB table\n",
    "table = dynamodb_resource.Table(dynamotable)\n",
    "try:\n",
    "    with table.batch_writer() as batch:\n",
    "        for user_id in user_ids:\n",
    "            batch.delete_item(Key={'user_id': user_id})\n",
    "    print(\"All items deleted from DynamoDB table.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting items from DynamoDB table: {e}\")\n",
    "\n",
    "print(\"Clean-up process completed. Please check the AWS Console to ensure all resources have been properly deleted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
